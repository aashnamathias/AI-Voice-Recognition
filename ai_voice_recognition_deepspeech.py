# -*- coding: utf-8 -*-
"""AI_Voice_Recognition_DeepSpeech.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1xC7GPcV09cnnkslOVA6UcLz9tBNhYMWN
"""

!pip install transformers
!pip install datasets
!pip install torchaudio
!pip install librosa
!pip install jiwer

!pip install fsspec==2025.3.2
!pip install torch==2.4.0 torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

!wget https://www.voiptroubleshooter.com/open_speech/american/OSR_us_000_0010_8k.wav -O audio.wav

!ffmpeg -i audio.wav -ar 16000 -ac 1 audio_16k.wav

from IPython.display import Audio
Audio("audio_16k.wav")

import json

dataset = [{
    "path": "audio_16k.wav",
    "text": "The birch canoe slid on the smooth planks. Glue the sheet to the dark blue background. It's easy to tell the depth of a well. These days a chicken leg is a rare dish. Rice is often served in round bowls. The juice of lemons makes fine punch. The box was thrown beside the parked truck. The hogs were fed chopped corn and garbage. Four hours of steady work faced us. A large size in stockings is hard to sell."
}]

with open("my_dataset.json", "w") as f:
    json.dump(dataset, f, indent=4)

from datasets import load_dataset, Dataset
import pandas as pd

data = pd.read_json("my_dataset.json")
ds = Dataset.from_pandas(data)

import torchaudio

def speech_file_to_array_fn(batch):
    speech_array, sampling_rate = torchaudio.load(batch["path"])
    batch["speech"] = speech_array[0].numpy()
    batch["sampling_rate"] = sampling_rate
    batch["target_text"] = batch["text"]
    return batch

ds = ds.map(speech_file_to_array_fn)

import torchaudio

def speech_file_to_array_fn(path):
    speech_array, sampling_rate = torchaudio.load(path)
    return speech_array[0].numpy(), sampling_rate

speech, sampling_rate = speech_file_to_array_fn("audio_16k.wav")

from datasets import Dataset

data = {
    "speech": [speech],
    "sampling_rate": [sampling_rate],
    "target_text": ["The birch canoe slid on the smooth planks. Glue the sheet to the dark blue background. It's easy to tell the depth of a well. These days a chicken leg is a rare dish. Rice is often served in round bowls. The juice of lemons makes fine punch. The box was thrown beside the parked truck. The hogs were fed chopped corn and garbage. Four hours of steady work faced us. A large size in stockings is hard to sell."]
}

ds = Dataset.from_dict(data)

from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC

processor = Wav2Vec2Processor.from_pretrained("facebook/wav2vec2-base-960h")
model = Wav2Vec2ForCTC.from_pretrained("facebook/wav2vec2-base-960h")

import numpy as np
feature_extractor = processor.feature_extractor

test_audio = np.array(ds[0]["speech"], dtype=np.float32)
test_output = feature_extractor(
    test_audio,
    sampling_rate=16000,
    return_tensors="pt"
)

print(test_output.input_values.shape)

def prepare_dataset(batch):
    speech_array = np.array(batch["speech"], dtype=np.float32)

    input_values = processor.feature_extractor(
        speech_array,
        sampling_rate=batch["sampling_rate"],
        return_tensors="pt"
    ).input_values[0]

    with processor.as_target_processor():
        labels = processor.tokenizer(
            batch["target_text"],
            return_tensors="pt",
            padding=True
        ).input_ids[0]

    return {"input_values": input_values, "labels": labels}

ds = ds.map(prepare_dataset)

from transformers import (
    Wav2Vec2ForCTC,
    Wav2Vec2Processor,
    Trainer,
    TrainingArguments
)
import numpy as np
import torch
from datasets import load_dataset, Audio

if len(ds) > 1:
    ds = ds.train_test_split(test_size=0.2)

from torch.utils.data import DataLoader

def collate_fn(batch):
    input_values = [example["input_values"] for example in batch]
    labels = [example["labels"] for example in batch]

    input_values = processor.feature_extractor.pad(
        {"input_values": input_values},
        return_tensors="pt"
    ).input_values

    with processor.as_target_processor():
        labels = processor.tokenizer.pad(
            {"input_ids": labels},
            return_tensors="pt"
        ).input_ids

    return {"input_values": input_values, "labels": labels}

train_dataloader = DataLoader(ds, batch_size=2, collate_fn=collate_fn)

!pip install transformers torchaudio librosa jiwer

!ffmpeg -i audio.wav -ar 16000 -ac 1 audio_16k.wav -y

import torchaudio
import numpy as np
import torch
from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor

# Load model and processor (BEST CHOICE)
processor = Wav2Vec2Processor.from_pretrained("facebook/wav2vec2-large-960h-lv60-self")
model = Wav2Vec2ForCTC.from_pretrained("facebook/wav2vec2-large-960h-lv60-self")

# Load and normalize audio
speech_array, sampling_rate = torchaudio.load("audio_16k.wav")
speech_array = speech_array / torch.abs(speech_array).max()

# Process
inputs = processor(speech_array[0], sampling_rate=16000, return_tensors="pt")

# Predict
with torch.no_grad():
    logits = model(**inputs).logits
predicted_ids = torch.argmax(logits, dim=-1)
transcription = processor.decode(predicted_ids[0])

print("Transcription:", transcription)

!pip install deepmultilingualpunctuation

from deepmultilingualpunctuation import PunctuationModel
import re

# Load punctuation model
punct_model = PunctuationModel()

# Add punctuation
punctuated = punct_model.restore_punctuation(transcription)

# --- Fix incorrect split in the first sentence ---

punctuated = re.sub(
    r"The birch canoe slid on\.\s+The smooth planks",
    "The birch canoe slid on the smooth planks. Glue the sheet",
    punctuated,
    flags=re.IGNORECASE
)

# Capitalize sentences properly
def capitalize_sentences(text):
    text = text.strip().capitalize()
    return re.sub(r'([.!?])\s+([a-z])', lambda m: m.group(1) + " " + m.group(2).upper(), text)

final_text = capitalize_sentences(punctuated)

# Output
print("\nFinal output with punctuation and capitalization:\n")
print(final_text)

from jiwer import wer

reference = "The actual reference text here"
hypothesis = transcription  # your model's output

error = wer(reference.lower(), hypothesis.lower())
print(f"WER (Word Error Rate): {error:.2f}")

!pip install gradio transformers torchaudio --quiet

import gradio as gr
import torchaudio
import torch
from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC

# Load model & processor once
processor = Wav2Vec2Processor.from_pretrained("facebook/wav2vec2-large-960h-lv60-self")
model = Wav2Vec2ForCTC.from_pretrained("facebook/wav2vec2-large-960h-lv60-self")

def transcribe_from_audio(audio_path):
    # Load audio
    waveform, sr = torchaudio.load(audio_path)

    # Convert to mono if stereo
    if waveform.shape[0] > 1:
        waveform = waveform.mean(dim=0, keepdim=True)

    # Resample to 16kHz if needed
    if sr != 16000:
        resampler = torchaudio.transforms.Resample(sr, 16000)
        waveform = resampler(waveform)
        sr = 16000

    # Normalize waveform
    waveform = waveform / waveform.abs().max()

    # Process and predict
    inputs = processor(waveform.squeeze(), sampling_rate=sr, return_tensors="pt")
    with torch.no_grad():
        logits = model(**inputs).logits
    predicted_ids = torch.argmax(logits, dim=-1)
    transcription = processor.decode(predicted_ids[0])

    return transcription

# Gradio interface
gr.Interface(
    fn=transcribe_from_audio,
    inputs=gr.Audio(type="filepath", label="üé§ Upload or record your voice"),
    outputs=gr.Textbox(label="üìù Transcribed text"),
    title="üéôÔ∏è My Speech Recognizer with Wav2Vec2",
    description="Record or upload audio to transcribe using Facebook's Wav2Vec2 model."
).launch()

import torch
import torchaudio
import gradio as gr
from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor
from jiwer import wer

# Load model and processor
processor = Wav2Vec2Processor.from_pretrained("facebook/wav2vec2-base-960h")
model = Wav2Vec2ForCTC.from_pretrained("facebook/wav2vec2-base-960h")

def transcribe_and_evaluate(audio_path, reference_text):
    # Load audio
    waveform, sr = torchaudio.load(audio_path)

    # Convert stereo to mono
    if waveform.shape[0] > 1:
        waveform = waveform.mean(dim=0, keepdim=True)

    # Resample if needed
    if sr != 16000:
        resampler = torchaudio.transforms.Resample(sr, 16000)
        waveform = resampler(waveform)
        sr = 16000

    # Normalize
    waveform = waveform / waveform.abs().max()

    # Tokenize and predict
    inputs = processor(waveform.squeeze(), sampling_rate=sr, return_tensors="pt")
    with torch.no_grad():
        logits = model(**inputs).logits
    predicted_ids = torch.argmax(logits, dim=-1)
    predicted_text = processor.decode(predicted_ids[0])

    # Calculate WER
    error_rate = wer(reference_text.lower(), predicted_text.lower())

    return predicted_text, f"{error_rate:.2%} WER"

# Gradio interface
gr.Interface(
    fn=transcribe_and_evaluate,
    inputs=[
        gr.Audio(type="filepath", label="Upload or record your voice"),
        gr.Textbox(label="Ground Truth Text (what was actually said in capitals with no punctuations)")
    ],
    outputs=[
        gr.Textbox(label="Transcribed Text"),
        gr.Textbox(label="Word Error Rate")
    ],
    title="Speech Recognizer with WER",
    description="Upload/record audio and enter what was actually said. This app will transcribe and show WER score."
).launch()

from datasets import load_dataset

dataset = load_dataset("DTU54DL/common-accent")
print(dataset)

from datasets import Audio

# Assuming the audio column is named "audio"
dataset = dataset.cast_column("audio", Audio(sampling_rate=16000))

# Print one sample from the training set
print(dataset["train"][0])

from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor
import torch

# Load pre-trained Wav2Vec2 model
processor = Wav2Vec2Processor.from_pretrained("facebook/wav2vec2-large-960h-lv60-self")
model = Wav2Vec2ForCTC.from_pretrained("facebook/wav2vec2-large-960h-lv60-self")

import librosa

def preprocess_audio(array, sr):
    trimmed, _ = librosa.effects.trim(array)  # trims leading/trailing silence
    return trimmed

def transcribe_audio(sample):
    audio_array = preprocess_audio(sample["audio"]["array"], sample["audio"]["sampling_rate"])
    inputs = processor(audio_array, sampling_rate=16000, return_tensors="pt", padding=True)
    with torch.no_grad():
        logits = model(**inputs).logits
    predicted_ids = torch.argmax(logits, dim=-1)
    transcription = processor.decode(predicted_ids[0])
    return transcription

small_dataset = dataset["train"].select(range(20))
small_dataset = small_dataset.map(lambda x: {"predicted_transcription": transcribe_audio(x)})

for i in range(len(small_dataset)):
    print(f"Accent: {small_dataset[i]['accent']}")
    print(f"Original: {small_dataset[i]['sentence']}")
    print(f"Predicted: {small_dataset[i]['predicted_transcription']}")
    print("-" * 50)

!pip install evaluate
import evaluate

wer_metric = evaluate.load("wer")

references = [x["sentence"].lower() for x in small_dataset]
predictions = [x["predicted_transcription"].lower() for x in small_dataset]

wer_score = wer_metric.compute(predictions=predictions, references=references)

print(f"Word Error Rate (WER): {wer_score}")

from collections import defaultdict

accent_groups = defaultdict(list)
for example in small_dataset:
    accent_groups[example['accent']].append((example['sentence'], example['predicted_transcription']))

for accent, samples in accent_groups.items():
    print(f"Accent: {accent}")
    refs = [s.lower() for s, _ in samples]
    preds = [p.lower() for _, p in samples]
    wer = wer_metric.compute(predictions=preds, references=refs)
    print(f"  WER: {wer:.3f}\n")

!pip install streamlit
!npm install -g localtunnel

!pip install langdetect
!pip install transformers

# Commented out IPython magic to ensure Python compatibility.
# %%writefile app.py
# import streamlit as st
# from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC
# from deepmultilingualpunctuation import PunctuationModel
# import torch
# import torchaudio
# import tempfile
# import re
# 
# st.title("üéôÔ∏è Voice Recognition")
# 
# @st.cache_resource
# def load_model():
#     processor = Wav2Vec2Processor.from_pretrained("facebook/wav2vec2-large-960h-lv60-self")
#     model = Wav2Vec2ForCTC.from_pretrained("facebook/wav2vec2-large-960h-lv60-self")
#     return processor, model
# 
# @st.cache_resource
# def load_punct_model():
#     return PunctuationModel()
# 
# processor, model = load_model()
# punct_model = load_punct_model()
# 
# uploaded_file = st.file_uploader("Upload a WAV file", type=["wav"])
# if uploaded_file is not None:
#     st.audio(uploaded_file)
# 
#     with tempfile.NamedTemporaryFile(delete=False) as tmp:
#         tmp.write(uploaded_file.read())
#         tmp_path = tmp.name
# 
#     speech_array, sampling_rate = torchaudio.load(tmp_path)
#     resampler = torchaudio.transforms.Resample(orig_freq=sampling_rate, new_freq=16000)
#     speech = resampler(speech_array).squeeze().numpy()
# 
#     inputs = processor(speech, sampling_rate=16000, return_tensors="pt", padding=True)
# 
#     with st.spinner("Transcribing... please wait ‚è≥"):
#         with torch.no_grad():
#             logits = model(**inputs).logits
#         predicted_ids = torch.argmax(logits, dim=-1)
#         transcription = processor.decode(predicted_ids[0])
# 
#     st.markdown("### ‚úèÔ∏è Raw Transcription:")
#     st.success(transcription)
#     st.markdown(f"**üî¢ Word Count:** {len(transcription.split())}")
# 
#     with st.spinner("Restoring punctuation... ‚úçÔ∏è"):
#         punctuated_text = punct_model.restore_punctuation(transcription)
# 
#         # Capitalize the first word of each sentence
#         punctuated_text = re.sub(r'([.?!]\s+)([a-z])', lambda m: m.group(1) + m.group(2).upper(), punctuated_text)
#         punctuated_text = punctuated_text[0].upper() + punctuated_text[1:]
# 
#     st.markdown("### üìù Transcription with Punctuation:")
#     st.info(punctuated_text)

#Copy paste the output to "Tunnel Password" in streamlit
!curl https://loca.lt/mytunnelpassword

!streamlit run app.py & npx localtunnel --port 8501

